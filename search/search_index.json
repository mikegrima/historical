{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Historical This project is in very active development and is not yet ready for production use! Historical is a serverless application that tracks and reacts to AWS resource modifications anywhere in your environment. Historical achieves this by describing AWS resources when they are changed, and keeping the history of those changes along with the the CloudTrail context of those changes. Historical persists data in two places: A \"Current\" DynamoDB table, which is a cache of the current state of AWS resources A \"Durable\" DynamoDB table, which stores the change history of AWS resources Historical enables downstream consumers to react to changes in the AWS environment without the need to directly describe the resource. This greatly increases speed of reaction, reduces IAM permission complexity, and also avoids rate limiting. How it works \u00b6 Historical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function to describe the AWS resource that changed, and saves the configuration of the resource into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the resource has effectively changed from what was previously known about that resource. If the item has changed, a new change record is logged, which then enables downstream consumers the ability to react to changes in the environment as the environment effectively changes. The CloudTrail context on the change is preserved in the change history. Current Technologies Implemented \u00b6 S3 \u00b6 Security Groups \u00b6 IAM (In active development) \u00b6 Architecture \u00b6 Please review the Architecture documentation for an in-depth description of the components involved. Installation & Configuration \u00b6 Documentation is under development.","title":"Welcome"},{"location":"#how-it-works","text":"Historical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function to describe the AWS resource that changed, and saves the configuration of the resource into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the resource has effectively changed from what was previously known about that resource. If the item has changed, a new change record is logged, which then enables downstream consumers the ability to react to changes in the environment as the environment effectively changes. The CloudTrail context on the change is preserved in the change history.","title":"How it works"},{"location":"#current-technologies-implemented","text":"","title":"Current Technologies Implemented"},{"location":"#s3","text":"","title":"S3"},{"location":"#security-groups","text":"","title":"Security Groups"},{"location":"#iam-in-active-development","text":"","title":"IAM (In active development)"},{"location":"#architecture","text":"Please review the Architecture documentation for an in-depth description of the components involved.","title":"Architecture"},{"location":"#installation-configuration","text":"Documentation is under development.","title":"Installation &amp; Configuration"},{"location":"architecture/","text":"Historical Architecture \u00b6 Historical is a serverless AWS application that consists of many components. Historical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch. General Architectural Overview \u00b6 Here is a diagram of the Historical Architecture: Please Note: This stack is deployed for every technology monitored ! There are many, many Historical stacks that will be deployed. Polling vs. Events \u00b6 Historical is both a polling and event driven system. It will periodically poll AWS accounts for changes. However, because Historical responds to events in the environment, polling doesn't need to be very aggressive and only happens once every few hours. Polling is necessary because events are not 100% reliable. This ensures that data is current just in case an event is dropped. Historical is eventually consistent , and makes a best effort to maintain a current and up-to-date inventory of AWS resources. Prerequisites \u00b6 For all of this to work the following prerequisites that must be satisfied: ALL AWS accounts must be configured to send CloudWatch events over a CloudWatch Event Bus to the Historical AWS account. ALL AWS accounts and ALL regions in those accounts need to have a CloudWatch Event rule that captures ALL events and sends them over the Event Bus. ALL AWS accounts must have CloudTrail enabled. A Historical IAM role must exist in ALL AWS accounts with permissions that are defined here *(TODO ADD THESE!). This role must have an AssumeRolePolicyDocument to permit access from the HistoricalLambdaProfile IAM role in the Historical Account. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate. The CloudWatch configuration is outlined here: The IAM configuration is outlined here: TODO ADD ME Regions \u00b6 Historical has the concept of regions that fit 3 categories: Primary region Secondary region(s) Off region(s) The Primary Region is considered the \"Base\" of Historical. This region has all of the major components that make up Historical. This region is responsible for getting events from ALL the off-regions -- which are regions that don't require a full Historical stack, but should still have events processed. The Secondary Region(s) are regions that are important to you. Secondary regions look like the primary region, and process in-region events. If you have a lot of infrastructure within a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data. The Off Region(s) are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of infrastructure deployed. These regions will forward ALL events to the Primary Region for processing. Note: It is highly recommended to have a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment. Component Overview \u00b6 This section describes some of the high-level architectural components. Primary Components \u00b6 Below are the primary components of the Historical architecture: CloudWatch Event Rules CloudWatch Change Events Poller Collector Current Table DynamoDB Stream Proxy Differ Durable Table Off-region SNS forwarders As general overview, the infrastructure follows a pipeline from start to finish. An event will arrive, will get enriched with additional information, and will provide notifications to downstream infrastructure on the given changes. SQS queues are used in as many places as possible to trigger Lambda functions. SQS makes it easy to provide Lambda execution concurrency, with retry on failure as well as dead-letter queuing capabilities. SNS topics are used to make it easy for N number of interested parties to subscribe to the Historical DynamoDB tables as they become updated. Presently, this is only attached to the Durable table. More details on this below. CloudWatch Event Rules \u00b6 There are two different CloudWatch Event Rules: Timed Events Change Events Timed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from CloudWatch Events when an AWS resource's configuration changes. Poller \u00b6 The Poller's primary function is to obtain a full inventory of AWS assets. The Poller is split into two parts: Poller Tasker Poller The \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and will task the Poller to list all assets in the given environment. The Poller Tasker in the PRIMARY REGION will task the Poller to list assets that reside in the primary region and all off-regions. A Poller Tasker in a SECONDARY REGION will only task a poller to describe assets that reside in the same region. The Poller lists all assets in a given account/region, and will task a \"Poller Collector\" to fetch details about the asset in question. Collector \u00b6 The Collector's primary function is to describe a given AWS asset and store its configuration to the \"Current\" DynamoDB table. The Collector is split into two parts (same code, different execution triggers): Poller Collector Event Collector The Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events. The Collector is split into two parts in order to prevent change events from being sandwiched in between polling events. Historical will always try to give priority to change events over polling events to ensure timeliness of asset configuration changes. In both cases, the Collector will go to the AWS account and region that the item resides in, and use boto3 to describe the configuration of the asset. Current Table \u00b6 The \"Current\" table is a global DynamoDB table that stores the current configuration of a given asset in AWS. This acts as a cache for current the state of the environment. The Current table has as DynamoDB stream that will kick off the DynamoDB Stream Proxy, which will then trigger the Differ. Special Note: \u00b6 The Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration. DynamoDB Stream Proxy \u00b6 The DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table. The Historical infrastructure has two configurations for the DynamoDB forwarder: Current Table Forwarder (DynamoDB Stream Proxy to Differ SQS) Durable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS) The Current Table Forwarder proxies events to the SQS queue that triggers the Differ Lambda function. This is sent to SQS to speed up the time to trigger the Differ. SNS lacks a batch message sending API, and thus sending to SNS is slower as a result. The Durable Table Forwarder proxies events to an SNS topic that any downstream subscriber for effective infrastructure changes can react to. SNS enables N subscribers to events. Special Note: \u00b6 DynamoDB Streams in Global DynamoDB tables triggers this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. Thus, to avoid Historical Lambda functions from \"stepping on each other's toes\", the DynamoDB Stream Proxy has a PROXY_REGIONS environment variable. This variable (a comma-separated list of AWS regions) is configured to only proxy DynamoDB stream updates that occur to assets that are configured. The PRIMARY REGION will be configured to proxy events that occur in the primary region, and all off-regions. The SECONDARY REGION(S) will be configured to proxy events that occur in the same region. Another Special Note: \u00b6 DynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical API to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case). Differ \u00b6 The Differ is a Lambda function that gets triggered upon changes to the Current table. The Differ will check if the asset in question has had an effective change. If so, the Differ will save a new change record in the Durable table to maintain history of the asset as it changes over time, and will also save the CloudTrail context, including the time at which the event occurred. Durable Table \u00b6 The \"Durable\" table is a global DynamoDB table that stores an asset configuration with change history. The Durable table has as DynamoDB stream that will kick off another DynamoDB Stream Proxy, which will be used for interested parties to react to effective changes that occur in the environment. Off-Region SNS Forwarders \u00b6 In off-regions, very bare infrastructure is intentionally deployed. This helps to reduce costs and complexity of the Historical infrastructure. This is a SNS Topic that receives CloudWatch events for asset changes that occur in region. This topic forwards events to the Event Collector SQS queue in the primary region. Special Stacks \u00b6 Some asset types have different stack configurations due to nuances of the technology. The following technologies have different stack types: S3 S3 \u00b6 The AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace. For S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions. The secondary regions will still respond to in-region events, but lack all polling components. This diagram showcases the S3 stack. Installation & Configuration \u00b6 Please refer to the installation docs for additional details.","title":"Architecture"},{"location":"architecture/#historical-architecture","text":"Historical is a serverless AWS application that consists of many components. Historical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch.","title":"Historical Architecture"},{"location":"architecture/#general-architectural-overview","text":"Here is a diagram of the Historical Architecture: Please Note: This stack is deployed for every technology monitored ! There are many, many Historical stacks that will be deployed.","title":"General Architectural Overview"},{"location":"architecture/#polling-vs-events","text":"Historical is both a polling and event driven system. It will periodically poll AWS accounts for changes. However, because Historical responds to events in the environment, polling doesn't need to be very aggressive and only happens once every few hours. Polling is necessary because events are not 100% reliable. This ensures that data is current just in case an event is dropped. Historical is eventually consistent , and makes a best effort to maintain a current and up-to-date inventory of AWS resources.","title":"Polling vs. Events"},{"location":"architecture/#prerequisites","text":"For all of this to work the following prerequisites that must be satisfied: ALL AWS accounts must be configured to send CloudWatch events over a CloudWatch Event Bus to the Historical AWS account. ALL AWS accounts and ALL regions in those accounts need to have a CloudWatch Event rule that captures ALL events and sends them over the Event Bus. ALL AWS accounts must have CloudTrail enabled. A Historical IAM role must exist in ALL AWS accounts with permissions that are defined here *(TODO ADD THESE!). This role must have an AssumeRolePolicyDocument to permit access from the HistoricalLambdaProfile IAM role in the Historical Account. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate. The CloudWatch configuration is outlined here: The IAM configuration is outlined here: TODO ADD ME","title":"Prerequisites"},{"location":"architecture/#regions","text":"Historical has the concept of regions that fit 3 categories: Primary region Secondary region(s) Off region(s) The Primary Region is considered the \"Base\" of Historical. This region has all of the major components that make up Historical. This region is responsible for getting events from ALL the off-regions -- which are regions that don't require a full Historical stack, but should still have events processed. The Secondary Region(s) are regions that are important to you. Secondary regions look like the primary region, and process in-region events. If you have a lot of infrastructure within a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data. The Off Region(s) are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of infrastructure deployed. These regions will forward ALL events to the Primary Region for processing. Note: It is highly recommended to have a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment.","title":"Regions"},{"location":"architecture/#component-overview","text":"This section describes some of the high-level architectural components.","title":"Component Overview"},{"location":"architecture/#primary-components","text":"Below are the primary components of the Historical architecture: CloudWatch Event Rules CloudWatch Change Events Poller Collector Current Table DynamoDB Stream Proxy Differ Durable Table Off-region SNS forwarders As general overview, the infrastructure follows a pipeline from start to finish. An event will arrive, will get enriched with additional information, and will provide notifications to downstream infrastructure on the given changes. SQS queues are used in as many places as possible to trigger Lambda functions. SQS makes it easy to provide Lambda execution concurrency, with retry on failure as well as dead-letter queuing capabilities. SNS topics are used to make it easy for N number of interested parties to subscribe to the Historical DynamoDB tables as they become updated. Presently, this is only attached to the Durable table. More details on this below.","title":"Primary Components"},{"location":"architecture/#cloudwatch-event-rules","text":"There are two different CloudWatch Event Rules: Timed Events Change Events Timed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from CloudWatch Events when an AWS resource's configuration changes.","title":"CloudWatch Event Rules"},{"location":"architecture/#poller","text":"The Poller's primary function is to obtain a full inventory of AWS assets. The Poller is split into two parts: Poller Tasker Poller The \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and will task the Poller to list all assets in the given environment. The Poller Tasker in the PRIMARY REGION will task the Poller to list assets that reside in the primary region and all off-regions. A Poller Tasker in a SECONDARY REGION will only task a poller to describe assets that reside in the same region. The Poller lists all assets in a given account/region, and will task a \"Poller Collector\" to fetch details about the asset in question.","title":"Poller"},{"location":"architecture/#collector","text":"The Collector's primary function is to describe a given AWS asset and store its configuration to the \"Current\" DynamoDB table. The Collector is split into two parts (same code, different execution triggers): Poller Collector Event Collector The Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events. The Collector is split into two parts in order to prevent change events from being sandwiched in between polling events. Historical will always try to give priority to change events over polling events to ensure timeliness of asset configuration changes. In both cases, the Collector will go to the AWS account and region that the item resides in, and use boto3 to describe the configuration of the asset.","title":"Collector"},{"location":"architecture/#current-table","text":"The \"Current\" table is a global DynamoDB table that stores the current configuration of a given asset in AWS. This acts as a cache for current the state of the environment. The Current table has as DynamoDB stream that will kick off the DynamoDB Stream Proxy, which will then trigger the Differ.","title":"Current Table"},{"location":"architecture/#special-note","text":"The Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration.","title":"Special Note:"},{"location":"architecture/#dynamodb-stream-proxy","text":"The DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table. The Historical infrastructure has two configurations for the DynamoDB forwarder: Current Table Forwarder (DynamoDB Stream Proxy to Differ SQS) Durable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS) The Current Table Forwarder proxies events to the SQS queue that triggers the Differ Lambda function. This is sent to SQS to speed up the time to trigger the Differ. SNS lacks a batch message sending API, and thus sending to SNS is slower as a result. The Durable Table Forwarder proxies events to an SNS topic that any downstream subscriber for effective infrastructure changes can react to. SNS enables N subscribers to events.","title":"DynamoDB Stream Proxy"},{"location":"architecture/#special-note_1","text":"DynamoDB Streams in Global DynamoDB tables triggers this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. Thus, to avoid Historical Lambda functions from \"stepping on each other's toes\", the DynamoDB Stream Proxy has a PROXY_REGIONS environment variable. This variable (a comma-separated list of AWS regions) is configured to only proxy DynamoDB stream updates that occur to assets that are configured. The PRIMARY REGION will be configured to proxy events that occur in the primary region, and all off-regions. The SECONDARY REGION(S) will be configured to proxy events that occur in the same region.","title":"Special Note:"},{"location":"architecture/#another-special-note","text":"DynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical API to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case).","title":"Another Special Note:"},{"location":"architecture/#differ","text":"The Differ is a Lambda function that gets triggered upon changes to the Current table. The Differ will check if the asset in question has had an effective change. If so, the Differ will save a new change record in the Durable table to maintain history of the asset as it changes over time, and will also save the CloudTrail context, including the time at which the event occurred.","title":"Differ"},{"location":"architecture/#durable-table","text":"The \"Durable\" table is a global DynamoDB table that stores an asset configuration with change history. The Durable table has as DynamoDB stream that will kick off another DynamoDB Stream Proxy, which will be used for interested parties to react to effective changes that occur in the environment.","title":"Durable Table"},{"location":"architecture/#off-region-sns-forwarders","text":"In off-regions, very bare infrastructure is intentionally deployed. This helps to reduce costs and complexity of the Historical infrastructure. This is a SNS Topic that receives CloudWatch events for asset changes that occur in region. This topic forwards events to the Event Collector SQS queue in the primary region.","title":"Off-Region SNS Forwarders"},{"location":"architecture/#special-stacks","text":"Some asset types have different stack configurations due to nuances of the technology. The following technologies have different stack types: S3","title":"Special Stacks"},{"location":"architecture/#s3","text":"The AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace. For S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions. The secondary regions will still respond to in-region events, but lack all polling components. This diagram showcases the S3 stack.","title":"S3"},{"location":"architecture/#installation-configuration","text":"Please refer to the installation docs for additional details.","title":"Installation &amp; Configuration"},{"location":"installation/","text":"Installation & Configuration \u00b6 There is a lot to this... so please stay tuned!","title":"Installation and Configuration"},{"location":"installation/#installation-configuration","text":"There is a lot to this... so please stay tuned!","title":"Installation &amp; Configuration"}]}